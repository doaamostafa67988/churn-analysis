{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9497921,"sourceType":"datasetVersion","datasetId":5779904}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#**Business Objective**\nThe goal is to stop subscriber loss (churn) and increase revenue. By using data, we identify which users are likely to cancel and group them into \"behavior categories\" to make marketing **27%** more efficient.\n\n# **Approach & Methodology**\n1. **Churn Prediction**  \n   - Three classification models implemented:\n     - `Logistic Regression`\n     - `Decision Tree (max_depth=3)`\n     - `Random Forest (n_estimators=10, max_depth=3)`\n   - 80/20 train-test split\n   - Accuracy as primary metric\n\n2. **Subscriber Segmentation**  \n   - `K-means` clustering on engagement metrics\n   - Elbow method for optimal cluster count (k=3)\n","metadata":{"id":"LHQRPKV6J_Ad"}},{"cell_type":"markdown","source":"\n#**Data description**\n\n\n* **subscriber_id**: A unique identifier for each user. Acts as the primary key for the dataset.\n* **age_group**: The demographic bracket the user belongs to (e.g., 18-24, 25-34, 45+).\n* **engagement_time**: The total duration (usually in minutes or hours) a user has spent on the platform.\n* **engagement_frequency**: The number of times a user interacts with the service over a specific period (e.g., daily logins or weekly sessions).\n* **subscription_status**: The current state of the userâ€™s account (e.g., Active, Canceled, Expired, or Trial).","metadata":{"id":"cqXI8DCMV6QU"}},{"cell_type":"code","source":"# Import the necessary modules\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Specify the file path of your CSV file\nfile_path = \"/kaggle/input/combating-subscriber-churn-with-targeted-marketing/AZWatch_subscribers.csv\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(file_path)\ndf.head()","metadata":{"id":"9jHLRoy9-bj-","outputId":"30c67cdd-1ca6-4dbf-a266-f4effa730f91","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T13:13:42.167976Z","iopub.execute_input":"2026-01-19T13:13:42.168219Z","iopub.status.idle":"2026-01-19T13:13:47.315272Z","shell.execute_reply.started":"2026-01-19T13:13:42.168192Z","shell.execute_reply":"2026-01-19T13:13:47.314172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#**Data Preprocessing**\n\n1. **Splitting data**  \n   - Removed `subscriber_id` and `subscription_status` columns\n\n2. **Handled categorical variables**  \n   - One-hot encoded `age_group`  \n\n3. **Standardized numerical features**  \n   - `engagement_time`  \n   - `engagement_frequency`  ","metadata":{"id":"gRlZwmIHThnm"}},{"cell_type":"code","source":"# Separate predictor variables from class label\nX = df.drop(['subscriber_id','subscription_status'], axis=1)\ny = df.subscription_status\n","metadata":{"id":"wtKlPHgqB4qb","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T13:13:55.332615Z","iopub.execute_input":"2026-01-19T13:13:55.332962Z","iopub.status.idle":"2026-01-19T13:13:55.346160Z","shell.execute_reply.started":"2026-01-19T13:13:55.332934Z","shell.execute_reply":"2026-01-19T13:13:55.345140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split intro training and test sets (20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n                        X, y, test_size=.2, random_state=42)\n\n# Data processing: Apply One Hot Encoding on the categorical attribute: age_group\nX_train_prepared = pd.get_dummies(X_train, columns=['age_group'])\n\n# Data processing:Apply the same one hot encoding transformation on the test data\nX_test_prepared = pd.get_dummies(X_test, columns=['age_group'])\n","metadata":{"id":"Y5nG8cxkCI3i","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T13:13:57.160428Z","iopub.execute_input":"2026-01-19T13:13:57.160811Z","iopub.status.idle":"2026-01-19T13:13:57.178369Z","shell.execute_reply.started":"2026-01-19T13:13:57.160783Z","shell.execute_reply":"2026-01-19T13:13:57.177550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LOGISTIC REGRESSION CLASSIFIER\n# Train a logistic regression classifier for subscriber churn prediction\nmodel1 = LogisticRegression()\nmodel1.fit(X_train_prepared, y_train)\n\n# Calculate accuracy score of predictions on test set\nscore = model1.score(X_test_prepared, y_test)\nprint(\"\\nLogistic regression accuracy score: \", score)\n","metadata":{"id":"LeCFVMGUCNVJ","outputId":"0cedfd1a-bd7f-4f96-eb88-a13fbc0fd578","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T13:13:58.696364Z","iopub.execute_input":"2026-01-19T13:13:58.696719Z","iopub.status.idle":"2026-01-19T13:13:58.733861Z","shell.execute_reply.started":"2026-01-19T13:13:58.696692Z","shell.execute_reply":"2026-01-19T13:13:58.733098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DECISION TREE CLASSIFIER\n# Train a decision tree classifier for subscriber churn prediction\nmodel2 = DecisionTreeClassifier(max_depth=3, criterion=\"gini\")\nmodel2.fit(X_train_prepared, y_train)\n\n# Calculate decision tree's accuracy score of predictions on test set\nscore = model2.score(X_test_prepared, y_test)\nprint(\"\\nDecision tree accuracy score: \", score)\n","metadata":{"id":"ru89Ke--CPg5","outputId":"59c5f0ce-df0d-4eef-8f37-3a777a444382","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T13:14:01.887264Z","iopub.execute_input":"2026-01-19T13:14:01.887609Z","iopub.status.idle":"2026-01-19T13:14:01.902215Z","shell.execute_reply.started":"2026-01-19T13:14:01.887580Z","shell.execute_reply":"2026-01-19T13:14:01.900795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# RANDOM FOREST ENSEMBLE\n# Train a random forest ensemble classifier for subscriber churn prediction\nmodel3 = RandomForestClassifier(n_estimators = 10, max_depth=3)\nmodel3.fit(X_train_prepared, y_train)\n\n# Calculate ensemble's accuracy score of predictions on test set\nscore = model3.score(X_test_prepared, y_test)\nprint(\"\\nRandom Forest accuracy score: \", score)\n","metadata":{"id":"4xcIOvdlCXcD","outputId":"b02e2c40-8214-478a-aaaf-3d0faf27679b","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T13:14:05.850327Z","iopub.execute_input":"2026-01-19T13:14:05.850687Z","iopub.status.idle":"2026-01-19T13:14:05.886250Z","shell.execute_reply.started":"2026-01-19T13:14:05.850657Z","shell.execute_reply":"2026-01-19T13:14:05.885259Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#**Model Performance**\n| Model              | Accuracy |\n|--------------------|----------|\n| Logistic Regression| 92.5%    |\n| Decision Tree      | 92%      |\n| Random Forest      | 91.5%    |","metadata":{"id":"U26i_aETMMJN"}},{"cell_type":"code","source":"# SUBSCRIBER SEGMENTATION\n# You can optionally use a method like the elbow criterion and silhouette calculation to choose the number of clusters.\nsegmentation = X.drop(['age_group'], axis=1)\n\n# Scale the two numerical data attributes\nscaler = StandardScaler()\nscaler.fit(segmentation)\nsegmentation_normalized = scaler.transform(segmentation)\n\nsse = {} # sum of squared errors (distances) to each cluster\nfor k in range(1,20):\n    kmeans = KMeans(n_clusters=k, random_state=1)\n    kmeans.fit(segmentation_normalized)\n    sse[k] = kmeans.inertia_\n\nplt.title('Elbow method to choose k')\nplt.xlabel('k');plt.ylabel('SSE')\nsns.pointplot(x=list(sse.keys()), y=list(sse.values()))\nplt.show()\n","metadata":{"id":"8Oa7wzguF7BD","outputId":"30e4db3d-f379-4c14-df0b-89147eef30a8","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T13:14:10.173353Z","iopub.execute_input":"2026-01-19T13:14:10.173744Z","iopub.status.idle":"2026-01-19T13:14:10.639607Z","shell.execute_reply.started":"2026-01-19T13:14:10.173715Z","shell.execute_reply":"2026-01-19T13:14:10.638508Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The $k$ value with the highest score is generally considered the \"sweet spot\" for segmentation.","metadata":{"id":"yD9xIQFiQwIh"}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n# 1. Initialize a dictionary to store the quality score for each k\nscores = {}\n# 2. Test different cluster counts (from 2 groups up to 5)\nfor k in range(2, 6):\n   # Initialize the KMeans algorithm\n    kmeans = KMeans(n_clusters=k, random_state=1)\n\n    # Fit the model and assign each subscriber to a cluster\n    labels = kmeans.fit_predict(segmentation_normalized)\n\n    # Calculate the Silhouette Score (measures how well-defined the groups are)\n    score = silhouette_score(segmentation_normalized, labels)\n\n    # Save the score for this specific number of clusters\n    scores[k] = score\n# 3. Print the results in a clean table format for analysis\nfor k, value in scores.items():\n    print(f\"| {k} | {value:4f} |\")","metadata":{"id":"kZ6LHHdxGL6L","outputId":"4b12f421-0371-40b8-dc2d-1a3d7bdebaaf","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T13:14:13.432300Z","iopub.execute_input":"2026-01-19T13:14:13.432715Z","iopub.status.idle":"2026-01-19T13:14:13.642152Z","shell.execute_reply.started":"2026-01-19T13:14:13.432681Z","shell.execute_reply":"2026-01-19T13:14:13.641367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply k-means clustering with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=1)\nkmeans.fit_predict(segmentation_normalized)\n\n# Add cluster labels as a new attribute in the dataset before scaling\nsegmentation[\"cluster_id\"] = kmeans.labels_\n\n# Analyze average feature values and counts per cluster\nanalysis = segmentation.groupby(['cluster_id']).agg({\n    'engagement_time': ['mean'],\n    'engagement_frequency':['mean']\n}).round(0)\nanalysis","metadata":{"id":"TRy7_RoLn0n3","outputId":"dc4febea-3f46-4652-986a-16b849e15756","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T13:14:15.804825Z","iopub.execute_input":"2026-01-19T13:14:15.805160Z","iopub.status.idle":"2026-01-19T13:14:15.832025Z","shell.execute_reply.started":"2026-01-19T13:14:15.805132Z","shell.execute_reply":"2026-01-19T13:14:15.831018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Insights**\n### Subscriber Segmentation Results\n\n**Three distinct clusters identified:**\n\n| Cluster | Session Time | Session Interaction | Profile |\n| :--- | :--- | :--- | :--- |\n| 0 | 4 minutes | 5 | Light Users |\n| 1 | 7 minutes | 18 | High-Frequency Users |\n| 2 | 9 minutes | 9 | Moderate Users |\n","metadata":{"id":"7gLlXgktLsxr"}},{"cell_type":"markdown","source":"#**Recommendations**\n\n- **Light Users (Cluster 0):**  \n  1. Free premium content trials  \n  2. Personalized recommendations\n- **High-Frequency Users (Cluster 1):**  \n  1. Premium subscription tiers  \n  2. Ambassador programs  \n  3. Exclusive content access\n- **Moderate Users (Cluster 2):**  \n  1. Gamified learning features  \n  2. Curated course bundles\n\n","metadata":{"id":"97YbCSfwSqLk"}},{"cell_type":"markdown","source":"# **Future Work**\n1. Collect additional user behavior data\n2. Implement A/B testing for cluster-specific strategies\n3. Develop real-time churn prediction API\n4. Explore deep learning approaches for pattern detection\n","metadata":{"id":"WPzuFBgsRi_G"}}]}